{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSYvP0n8RjYH"
   },
   "source": [
    "# Frequent itemset mining\n",
    "\n",
    "- Student ID: 21127666\n",
    "- Student name: Trần Thuận Phát\n",
    "\n",
    "**How to do your homework**\n",
    "\n",
    "\n",
    "You will work directly on this notebook; the word `TODO` indicate the parts you need to do.\n",
    "\n",
    "You can discuss ideas with classmates as well as finding information from the internet, book, etc...; but *this homework must be your*.\n",
    "\n",
    "**How to submit your homework**\n",
    "\n",
    "Before submitting, rerun the notebook (`Kernel` ->` Restart & Run All`).\n",
    "\n",
    "Then create a folder named `ID` (for example, if your ID is 1234567, then name the folder `1234567`) Copy file notebook to this folder, compress and submit it on moodle.\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "- Frequent itemset mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXZ5gCVaRjYa"
   },
   "source": [
    "# 1. Preliminaries\n",
    "## This is how it all started ...\n",
    "- Rakesh Agrawal, Tomasz Imielinski, Arun N. Swami: Mining Association Rules between Sets of Items in Large Databases. SIGMOD Conference 1993: 207-216\n",
    "- Rakesh Agrawal, Ramakrishnan Srikant: Fast Algorithms for Mining Association Rules in Large Databases. VLDB 1994: 487-499\n",
    "\n",
    "**These two papers are credited with the birth of Data Mining**\n",
    "## Frequent itemset mining (FIM)\n",
    "\n",
    "Find combinations of items (itemsets) that occur frequently.\n",
    "## Applications\n",
    "- Items = products, transactions = sets of products someone bought in one trip to the store.\n",
    "$\\Rightarrow$ items people frequently buy together.\n",
    "    + Example: if people usually buy bread and coffee together, we run a sale of bread to attract people attention and raise price of coffee.\n",
    "- Items = webpages, transactions = words. Unusual words appearing together in a large number of documents, e.g., “Brad” and “Angelina,” may indicate an interesting relationship.\n",
    "- Transactions = Sentences, Items = Documents containing those sentences. Items that appear together too often could represent plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8vAJ8A2RjYi"
   },
   "source": [
    "## Transactional Database\n",
    "A transactional database $D$ consists of $N$ transactions: $D=\\left\\{T_1,T_2,...,T_N\\right\\}$. A transaction $T_n \\in D (1 \\le n \\le N)$ contains one or more items and that $I= \\left\\{ i_1,i_2,…,i_M \\right\\}$ is the set of distinct items in $D$, $T_n \\subset I$. Commonly, a transactional database is represented by a flat file instead of a database system: items are non-negative integers, each row represents a transaction, items in a transaction separated by space.\n",
    "\n",
    "Example: \n",
    "\n",
    "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \n",
    "\n",
    "30 31 32 \n",
    "\n",
    "33 34 35 \n",
    "\n",
    "36 37 38 39 40 41 42 43 44 45 46 \n",
    "\n",
    "38 39 47 48 \n",
    "\n",
    "38 39 48 49 50 51 52 53 54 55 56 57 58 \n",
    "\n",
    "32 41 59 60 61 62 \n",
    "\n",
    "3 39 48 \n",
    "\n",
    "63 64 65 66 67 68 \n",
    "\n",
    "\n",
    "\n",
    "# Definition\n",
    "\n",
    "- Itemset: A collection of one or more items.\n",
    "    + Example: {1 4 5}\n",
    "- **k-itemset**: An itemset that contains k items.\n",
    "- Support: Frequency of occurrence of an itemset.\n",
    "    + Example: From the example above, item 3 appear in 2 transactions so its support is 2.\n",
    "- Frequent itemset: An itemset whose support is greater than or equal to a `minsup` threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdykKxr6RjY-"
   },
   "source": [
    "# The Apriori Principle\n",
    "- If an itemset is frequent, then all of its subsets must also be frequent.\n",
    "- If an itemset is not frequent, then all of its supersets cannot be frequent.\n",
    "- The support of an itemset never exceeds the support of its subsets.\n",
    "$$ \\forall{X,Y}: (X \\subseteq Y) \\Rightarrow s(X)\\ge s(Y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvfMR7-CRjZB"
   },
   "source": [
    "# 2. Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9gZh4DORjZD"
   },
   "source": [
    "## The Apriori algorithm\n",
    "Suppose:\n",
    "\n",
    "$C_k$ candidate itemsets of size k.\n",
    "\n",
    "$L_k$ frequent itemsets of size k.\n",
    "\n",
    "The level-wise approach of Apriori algorithm can be descibed as follow:\n",
    "1. k=1, $C_k$ = all items.\n",
    "2. While $C_k$ not empty:\n",
    "    3. Scan the database to find which itemsets in $C_k$ are frequent and put them into $L_k$.\n",
    "    4. Use $L_k$ to generate a collection of candidate itemsets $C_{k+1}$ of size k+1.\n",
    "    5. k=k+1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF9xHOBLRjZJ"
   },
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7F0lUOSuRjZN"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OogwdcLRjZf"
   },
   "source": [
    "### Read data\n",
    "First we have to read data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2bsGrTERjZg"
   },
   "outputs": [],
   "source": [
    "\n",
    "def readData(path):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------------------\n",
    "        path: path of database D.\n",
    "         \n",
    "    --------------------------\n",
    "    Returns\n",
    "        data: a dictionary for representing database D\n",
    "                 - keys: transaction tids\n",
    "                 - values: itemsets.\n",
    "        s: support of distict items in D.\n",
    "    \"\"\"\n",
    "    data={}\n",
    "    s=defaultdict(lambda: 0) #* Initialize a dictionary for storing support of items in I.  \n",
    "    with open(path,'rt') as f:\n",
    "        tid=1\n",
    "        for line in f:\n",
    "            itemset=set(map(int,line.split())) #* A python set is a native way for storing an itemset.\n",
    "            for item in itemset:  \n",
    "                s[item]+=1     #* Why don't we compute support of items while reading data?\n",
    "            data[tid]= itemset\n",
    "            tid+=1\n",
    "    print(data)\n",
    "    return data, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSTC78WURjZu"
   },
   "source": [
    "### Tree Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGAkmuXtRjZw"
   },
   "source": [
    "**Question 0: I gave you pseudo code of Apriori algorithm above but we implement Tree Projection. Tell me the differences of two algorithms.**\n",
    "\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Data Structure:**\n",
    "\n",
    "- Apriori:\n",
    "    + Uses a candidate generation and testing approach.\n",
    "    + Involves multiple passes over the data to discover frequent itemsets.\n",
    "- Tree Projection:\n",
    "    + Utilizes an FP-tree (Frequent Pattern tree) data structure.\n",
    "    + Constructs a compact representation of frequent itemsets in a single pass.\n",
    "\n",
    "**Candidate Generation:**\n",
    "\n",
    "- Apriori:\n",
    "    + Generates candidate itemsets at each level and tests their support in the database.\n",
    "    + Involves multiple iterations of candidate generation and support counting.\n",
    "- Tree Projection:\n",
    "    + Does not explicitly generate candidate itemsets.\n",
    "    + Builds an FP-tree that directly represents frequent itemsets.\n",
    "\n",
    "**Pruning Strategy:**\n",
    "- Apriori:\n",
    "    + Prunes the search space by eliminating infrequent itemsets at each step.\n",
    "    + Involves multiple passes and may generate a large number of candidates.\n",
    "- Tree Projection:\n",
    "    + Prunes the search space by focusing on specific branches of the FP-tree.\n",
    "    + Can be more efficient in terms of both time and space, especially for dense datasets.\n",
    "\n",
    "**Number of Scans:**\n",
    "- Apriori:\n",
    "    + Typically requires multiple scans of the dataset to find frequent itemsets.\n",
    "    + Each pass involves candidate generation, support counting, and pruning.\n",
    "- Tree Projection:\n",
    "    + Constructs the FP-tree in a single pass over the dataset.\n",
    "    + Recursive mining is performed on the FP-tree to find frequent itemsets.\n",
    "\n",
    "**Memory Usage:**\n",
    "- Apriori:\n",
    "    + May require significant memory, especially for large candidate sets.\n",
    "- Tree Projection:\n",
    "    + Tends to be more memory-efficient due to the compact representation of frequent itemsets in the FP-tree.\n",
    "\n",
    "**Handling Large Datasets:**\n",
    "- Apriori:\n",
    "    + Can become inefficient for large datasets due to multiple passes and candidate generation.\n",
    "- Tree Projection:\n",
    "    + Generally more efficient for large datasets, as it constructs a concise data structure in a single pass.\n",
    "\n",
    "**Algorithm Complexity:**\n",
    "- Apriori:\n",
    "    + Can have higher time complexity, especially for datasets with a large number of candidates.\n",
    "- Tree Projection:\n",
    "    + Often exhibits better time complexity, making it more scalable for certain scenarios.\n",
    "\n",
    "In summary, while both algorithms aim to discover frequent itemsets in transactional databases, the Apriori algorithm relies on candidate generation and multiple passes, whereas the Tree Projection algorithm uses an FP-tree structure for a more efficient single-pass approach. The choice between the two depends on the characteristics of the dataset and the desired trade-offs between time and space complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVRT5BnWRjZz"
   },
   "outputs": [],
   "source": [
    "def joinset(a,b):\n",
    "    '''\n",
    "    Parameters\n",
    "    -------------------\n",
    "        2 itemsets a and b (of course they are at same branch in search space) \n",
    "\n",
    "    -------------------\n",
    "    return \n",
    "        ret: itemset generated by joining a and b\n",
    "    '''\n",
    "    # a,b same brach -> a_k = b_k\n",
    "    # Hint: this function will be called in generateSearchSpace method.:\n",
    "    new_set = sorted(set(a) | set(b))\n",
    "\n",
    "    return new_set\n",
    "    \n",
    "    \n",
    "class TP:\n",
    "    def __init__ (self, data=None, s=None, minSup=None):\n",
    "        self.data=data\n",
    "        self.s={}\n",
    "        \n",
    "        \n",
    "        for key, support in sorted(s.items(),key= lambda item: item[1]):\n",
    "            self.s[key]=support\n",
    "        # Why should we do this, answer it at the markdown below?\n",
    "            \n",
    "        self.minSup=minSup\n",
    "        self.L={}  #Store frequent itemsets mined from database\n",
    "        self.runAlgorithm()\n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Initialize search space at first step\n",
    "        --------------------------------------\n",
    "        We represent our search space in a tree structure\n",
    "        \"\"\"\n",
    "        tree={}\n",
    "\n",
    "        search_space={}  \n",
    "        for item, support in self.s.items():\n",
    "            search_space[item]={}\n",
    "\n",
    "            search_space[item]['itemset']=[item] \n",
    "            ''' \n",
    "            python set does not remain elements order\n",
    "            so we use a list to extend it easily when create new itemset \n",
    "            but why we store itemset in data by a python set???? '''\n",
    "            # TODO: study about python set and its advantages,\n",
    "            # answer at the markdown below.\n",
    "            \n",
    "            search_space[item]['pruned']=False\n",
    "\n",
    "            \n",
    "            search_space[item]['support']=support\n",
    "            \n",
    "            tree[item]={}\n",
    "            '''\n",
    "            Why should i store an additional tree (here it called tree)? \n",
    "            Answer: This really help in next steps.\n",
    "            \n",
    "            Remember that there is always a big gap from theory to practicality\n",
    "            and implementing this algorithm in python is not as simple as you think.\n",
    "            ''' \n",
    "        \n",
    "        return tree, search_space\n",
    "    \n",
    "    def computeItemsetSupport(self, itemset):\n",
    "        \n",
    "        '''Return support of itemset'''\n",
    "        # TODO (hint: this is why i use python set in data)\n",
    "        support = 0\n",
    "        for transaction in self.data:\n",
    "            if set(itemset).issubset(self.data[transaction]):\n",
    "                support += 1\n",
    "\n",
    "        return support\n",
    "    \n",
    "    \n",
    "        \n",
    "    def prune(self,k, tree, search_space):\n",
    "        \n",
    "        '''\n",
    "        In this method we will find out which itemset in current search space is frequent \n",
    "        itemset then add it to L[k]. In addition, we prune those are not frequent itemsets. \n",
    "        '''\n",
    "        #TODO\n",
    "        for item in search_space:\n",
    "            if not (item == 'pruned' or item == 'itemset' or item == 'support'):\n",
    "                support = self.computeItemsetSupport(search_space[item]['itemset'])\n",
    "                if support >= self.minSup:\n",
    "                    if k not in self.L:\n",
    "                        self.L[k] = []\n",
    "                    self.L[k].append(search_space[item]['itemset'])\n",
    "                else:\n",
    "                    search_space[item]['pruned']=True\n",
    "       \n",
    "    def generateSearchSpace(self,k, tree, search_space):\n",
    "        '''\n",
    "        Generate search space for exploring k+1 itemset. (Recursive function) \n",
    "        '''\n",
    "        items=list(tree.keys())  \n",
    "        ''' print search_space.keys() you will understand  \n",
    "         why we need an additional tree, '''\n",
    "        l=len(items)\n",
    "        self.prune(k, tree, search_space)\n",
    "        if l==0: return   #Stop condition\n",
    "        \n",
    "        for i in range(l-1):\n",
    "            a=items[i]\n",
    "            if search_space[a]['pruned']: continue \n",
    "                \n",
    "            for j in range(i+1,l):\n",
    "                b=items[j]\n",
    "                \n",
    "                search_space[a][b]={}\n",
    "                tree[a][b]={}\n",
    "                #TODO\n",
    "                # You really need to understand what am i doing here before doing work below.\n",
    "                # (Hint: draw tree and search space to draft). \n",
    "            \n",
    "                #First create newset using join set\n",
    "                new_set =  joinset(search_space[a]['itemset'], search_space[b]['itemset'])\n",
    "\n",
    "                #Second add newset to search_space\n",
    "                search_space[a][b]['itemset'] = new_set\n",
    "                search_space[a][b]['pruned'] = False\n",
    "                search_space[a][b]['support'] = self.computeItemsetSupport(new_set)\n",
    "                \n",
    "            #  Generate search_space for k+1-itemset\n",
    "            self.generateSearchSpace(k+1,tree[a],search_space[a])\n",
    "\n",
    "    def runAlgorithm(self):\n",
    "        tree,search_space=self.initialize() #generate search space for 1-itemset\n",
    "        self.generateSearchSpace(1, tree, search_space)\n",
    "    def miningResults(self):\n",
    "        return self.L\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tMTpwxLRjZ-"
   },
   "source": [
    "Ok, let's test on a typical dataset `chess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLygYqiYRjZ-"
   },
   "outputs": [],
   "source": [
    "transactions, freq= readData('chess.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PnxbU77YRjaF"
   },
   "outputs": [],
   "source": [
    "# Run and print result (better print format)\n",
    "a=TP(data=transactions,s=freq, minSup=3000)\n",
    "print(*a.miningResults().items(),sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp0RFbw-RjaU"
   },
   "source": [
    "### Answer questions here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1. Why should we sort all items in s by it's support and why we have to restored it to new artribute s?**\n",
    "\n",
    "**Answer:**\n",
    "- Sorting the items in s by support is likely done to process the items in ascending order of support during the algorithm execution. This helps in efficiently traversing and processing the search space and tree structures. By sorting, the algorithm can start with the least supported items, progressively building up to more supported ones, which can lead to better pruning opportunities and potentially faster execution.\n",
    "\n",
    "- The sorting operation itself modifies the original dictionary s. The reason for restoring it to a new attribute self.s is to maintain the original order of items in the dictionary. Python dictionaries do not guarantee any specific order of items, but by sorting and storing the result in a new attribute, the algorithm retains the sorted order for later use.\n",
    "\n",
    "**Question 2. Python set does not remain elements order so we use a list to extend it easily when create new itemset but why we store itemset in data by a python set????**\n",
    "\n",
    "**Answer:**\n",
    "- Storing itemsets in data as Python sets is likely for efficient membership testing during support computation. When checking whether an itemset is present in a transaction, using sets allows for constant-time membership tests, which is faster than iterating through a list or another data structure. This is important when computing the support of itemsets, as it involves checking for the presence of the itemset in each transaction.\n",
    "\n",
    "- On the other hand, when creating new itemsets (e.g., in the joinset function), a list is used because the order of elements matters during the generation of new itemsets. Lists maintain the order of elements, ensuring consistency in the representation of itemsets.\n",
    "\n",
    "\n",
    "**Question 3.  After finish implementing the algorithm tell me why should you use this instead of delete item directly from search_space and tree.**\n",
    "\n",
    "**Answer:**\n",
    "- In the Tree Projection algorithm, maintaining the original search_space and tree structures without direct deletion allows for a clearer understanding of the algorithm's progression and facilitates backtracking when necessary. Deleting items directly from these structures could lead to a loss of information and make it more challenging to backtrack or recover previous states during the recursive exploration of the search space.\n",
    "\n",
    "- By marking items as pruned in the search_space and retaining the original structures, the algorithm preserves the ability to explore different branches of the search space efficiently. This is crucial for the algorithm's correctness and ensures that the pruning decisions are reversible if needed. Additionally, keeping the original structures intact aids in debugging and understanding the algorithm's behavior at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnVm8wYIRjaV"
   },
   "source": [
    "# 3. Churn analysis\n",
    "\n",
    "In this section, you will use frequent itemset mining technique to analyze `churn` dataset (for any purposes). You can download dataset from here: http://ce.sharif.edu/courses/85-86/1/ce925/assignments/files/assignDir2/churn.txt. Write your report and implementation below.\n",
    "\n",
    "*Remember this dataset is not represented as a transactional database, first thing that you have to do is transforming it into a flat file.  \n",
    "More information about `churn` here: http://ce.sharif.edu/courses/85-86/1/ce925/assignments/files/assignDir4/Churn.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9U08alVRjaW"
   },
   "source": [
    "**TODO:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming dataset into a flat file and normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# The path to file churn.txt\n",
    "path = 'churn.txt'\n",
    "\n",
    "# Read the dataframe\n",
    "df = pd.read_csv(path, delimiter=',')\n",
    "\n",
    "# The number for each of columns\n",
    "print(\"- Unique values in column:\")\n",
    "for column in df.columns:\n",
    "    unique_values = df[column].nunique()\n",
    "    print(f\"  + '{column}': {unique_values}\")\n",
    "\n",
    "# we need to change value of Phone to get first three character\n",
    "print(\"\\n- Unique values in the 'Phone' column after the changes:\")\n",
    "df[\"Phone\"] = df[\"Phone\"].str.slice(0, 3).astype(str)\n",
    "print(f\"  + 'Phone': {df['Phone'].nunique()}\\n\")\n",
    "\n",
    "# Distinguish each of columns by adding the name of column.\n",
    "for column in df.columns:\n",
    "    df[column] = column + ' _ ' + df[column].astype(str)\n",
    "\n",
    "# Split the data into two part: First is true and second is false and drop the column Churn?\n",
    "true_df = df[df[\"Churn?\"] == \"Churn? _ True.\"].drop(\"Churn?\", axis=1)\n",
    "false_df = df[df[\"Churn?\"] == \"Churn? _ False.\"].drop(\"Churn?\", axis=1)\n",
    "\n",
    "# Convert to flat file\n",
    "true_flat_file = true_df.values\n",
    "true_flat_file = true_flat_file.astype(str)\n",
    "false_flat_file = false_df.values\n",
    "false_flat_file = false_flat_file.astype(str)\n",
    "\n",
    "# Ouput data\n",
    "print(\"\\n- True Flat file: \")\n",
    "print(true_flat_file)\n",
    "print(\"\\n- False Flat file: \")\n",
    "print(false_flat_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data and support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle the true case\n",
    "data_true={}\n",
    "s_true=defaultdict(lambda: 0)\n",
    "\n",
    "tid = 1\n",
    "for row in true_flat_file:\n",
    "    itemset = set(map(str, row))\n",
    "\n",
    "    # Update item support counts\n",
    "    for item in itemset:\n",
    "        s_true[item] += 1\n",
    "\n",
    "    # Store transaction in the data dictionary\n",
    "    data_true[tid] = itemset\n",
    "    tid += 1\n",
    "\n",
    "# Handle the false case\n",
    "data_false={}\n",
    "s_false=defaultdict(lambda: 0)\n",
    "\n",
    "tid = 1\n",
    "for row in false_flat_file:\n",
    "    itemset = set(map(str, row))\n",
    "\n",
    "    # Update item support counts\n",
    "    for item in itemset:\n",
    "        s_false[item] += 1\n",
    "\n",
    "    # Store transaction in the data dictionary\n",
    "    data_false[tid] = itemset\n",
    "    tid += 1\n",
    "\n",
    "# Ouput the result\n",
    "print(f\"Tramsaction of the true case: \\n{data_true}\\n\")\n",
    "print(f\"Tramsaction of the false case: \\n{data_false}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Tree Precision algorithm above to get the frequent itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and print the true case (better print format)\n",
    "print(\"- Result of the true case: \")\n",
    "a=TP(data=data_true,s=s_true, minSup=len(data_true) * 80 / 100)\n",
    "print(*a.miningResults().items(),sep=\"\\n\")\n",
    "\n",
    "# Run and print the false case (better print format)\n",
    "print(\"\\n- Result of the false case: \")\n",
    "a=TP(data=data_false,s=s_false, minSup=len(data_false) * 80 / 100)\n",
    "print(*a.miningResults().items(),sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate accuracy, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(TP, TN, FN, FP):\n",
    "    accuracy = (TP + TN) / (TP + TN + FN + FP)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1_score = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    return accuracy, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the results\n",
    "- Case 1: VMail Plan = 'no' ---> Churn? = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path, delimiter=',').astype(str)\n",
    "\n",
    "correct = 0\n",
    "sum_row = 0\n",
    "for index, row in df.iterrows():\n",
    "    if row['VMail Plan'] == 'no':\n",
    "        sum_row += 1\n",
    "    if row['VMail Plan'] == 'no' and row['Churn?'] == 'True.':\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy rate for case 1: {correct / sum_row}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Case 2: VMail Message = 0 ---> Churn? = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path, delimiter=',').astype(str)\n",
    "\n",
    "correct = 0\n",
    "sum_row = 0\n",
    "for index, row in df.iterrows():\n",
    "    if row['VMail Message'] == '0':\n",
    "        sum_row += 1\n",
    "    if row['VMail Message'] == '0' and row['Churn?'] == 'True.':\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy rate for case 2: {correct / sum_row}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rule 3 VMail Plan = 'no' and VMail Message = 0 ---> Churn? = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path, delimiter=',').astype(str)\n",
    "\n",
    "correct = 0\n",
    "sum_row = 0\n",
    "for index, row in df.iterrows():\n",
    "    if row['VMail Message'] == '0' and row['VMail Plan'] == 'no':\n",
    "        sum_row += 1\n",
    "    if row['VMail Message'] == '0' and row['VMail Plan'] == 'no' and row['Churn?'] == 'True.':\n",
    "        correct += 1\n",
    "        \n",
    "print(f\"Accuracy rate for case 3: {correct / sum_row}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Case 4: Int'l Plan = 'no' ---> Churn? = False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path, delimiter=',').astype(str)\n",
    "\n",
    "correct = 0\n",
    "sum_row = 0\n",
    "for index, row in df.iterrows():\n",
    "    if row[\"Int'l Plan\"] == 'no':\n",
    "        sum_row += 1\n",
    "    if row[\"Int'l Plan\"] == 'no' and row['Churn?'] == 'False.':\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy rate for case 4: {correct / sum_row}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Case 5: Apply all the cases above using f1_score, accuracy, precision, and recall to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path, delimiter=',').astype(str)\n",
    "# Determine only the rows that contain the values being considered in the mentioned cases\n",
    "df = df.loc[\n",
    "    (((df['VMail Message'] == '0') | (df['VMail Plan'] == 'no')) & (df[\"Int'l Plan\"] != 'no')) |\n",
    "    (((df[\"Int'l Plan\"] == 'no') & (df['VMail Message'] != '0') & (df['VMail Plan'] != 'no')))\n",
    "]\n",
    "\n",
    "TP = 0\n",
    "TN = 0\n",
    "FN = 0\n",
    "FP = 0\n",
    "for index, row in df.iterrows():\n",
    "    if (row['VMail Message'] == '0' or row['VMail Plan'] == 'no') and row['Churn?'] == 'True.':\n",
    "        TP += 1\n",
    "    elif row[\"Int'l Plan\"] == 'no' and row['Churn?'] == 'False.':\n",
    "        TN += 1\n",
    "    elif row[\"Int'l Plan\"] == 'no' and row['Churn?'] == 'True.':\n",
    "        FN += 1\n",
    "    else:\n",
    "        FP += 1\n",
    "\n",
    "accuracy, precision, recall, f1_score = calculate(TP, TN, FN, FP)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1_score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Churn rate distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the number of 'True.' and 'False.' in the 'Churn?' column\n",
    "true_count = df[df['Churn?'] == 'True.'].shape[0]\n",
    "false_count = df[df['Churn?'] == 'False.'].shape[0]\n",
    "\n",
    "# Calculate the churn_true_percentage\n",
    "total_count = len(df)\n",
    "churn_true_percentage = (true_count / total_count) * 100\n",
    "\n",
    "# Calculate the churn_false_percentage (just for illustration, not necessarily needed)\n",
    "churn_false_percentage = (false_count / total_count) * 100\n",
    "\n",
    "# Data and labels for the pie chart\n",
    "labels = ['Churn? = True.', 'Churn? = False.']\n",
    "sizes = [churn_true_percentage, churn_false_percentage]\n",
    "\n",
    "# Colors for the pie chart segments\n",
    "colors = ['#ff9999', '#66b3ff']\n",
    "\n",
    "# Draw the pie chart\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "\n",
    "# Add a title to the chart\n",
    "plt.title('Churn Rate Distribution')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before transforming dataset into a flat file, i need to analyze and observe the data, and then normalize the data.\n",
    "\n",
    "    + Above, we can see the unique values in each column. A special note is that the \"Phone\" column has exactly 3333 unique values, equal to the number of rows in the dataset. This makes the support for each value in it 1 (rendering it useless). Therefore, we only take the first 3 digits of each value in the \"Phone\" column, representing the code phone. After this change, the \"Phone\" column now has only 96 unique values.\n",
    "    \n",
    "    + Next, we convert all values in the dataset into strings and append the name of the corresponding column to the beginning of each value. Since some column values in the dataset are similar to values in other columns, this step ensures that each column has a distinct meaning and is treated as an item in an itemset.\n",
    "\n",
    "- Next, i have to transform the dataset into a flat file. Additionally, we will split the dataset into two parts: \n",
    "\n",
    "    + Part 1: contains the dataset with the \"Churn?\" column having the value True.\n",
    "\n",
    "    + Part 2: contains the dataset with the \"Churn?\" column having the value False.\n",
    "\n",
    "    + And then remove the \"Churn?\" column.\n",
    "    \n",
    "    + Reason: The goal here is to investigate the relationship between the \"Churn?\" column and other attributes. Specifically, i want to determine if users requesting churn (Churn? = True) are associated with items satisfying minSup. Conversely, if users do not request churn (Churn? = False), i want to identify the items associated with them that satisfy minSup. This process helps generate rules and make predictions.\n",
    "    \n",
    "- After obtaining the two datasets, True and False, i find transactions and support for each dataset and perform the tree precision algorithm to obtain frequent itemsets with minsup = 80% of the length of the corresponding dataset (meaning appearing in 80% or more of the rows in the dataset) for True and False.\n",
    "\n",
    "- After running the algorithm, i get the following results:\n",
    "\n",
    "    + Result of the true case:\n",
    "\n",
    "        (1, [['VMail Plan _ no'], ['VMail Message _ 0']])\n",
    "\n",
    "        (2, [['VMail Message _ 0', 'VMail Plan _ no']])\n",
    "\n",
    "    + Result of the false case:\n",
    "    \n",
    "        (1, [[\"Int'l Plan _ no\"]])\n",
    "\n",
    "- From this, i derive the following four rules:\n",
    "\n",
    "    + Rule 1: VMail Plan = 'no' ---> Churn? = True.\n",
    "\n",
    "    + Rule 2: VMail Message = '0' ---> Churn? = True.\n",
    "\n",
    "    + Rule 3 VMail Plan = 'no' and VMail Message = '0' ---> Churn? = True.\n",
    "\n",
    "    + Rule 4: Int'l Plan = 'no' ---> Churn? = False.\n",
    "\n",
    "- Next, i evaluate the accuracy of these rules based on the given dataset using the formula:\n",
    "\n",
    "                             accuracy = (correct) / sum_row\n",
    "\n",
    "    + correct: the number of rows where the rule's condition is true and the prediction is accurate.\n",
    "    \n",
    "    + sum_row: the total number of rows where the rule's condition is true.\n",
    "\n",
    "\n",
    "\n",
    "- For cases 1, 2, and 3:\n",
    "\n",
    "    + I have accuracy = 0.1671505599336375 (indicating that these cases have low accuracy and are not reliable when applying these rules to the dataset for prediction).\n",
    "    \n",
    "    <br>\n",
    "\n",
    "- For case 4:\n",
    "\n",
    "    + I have accuracy = 0.8850498338870432 (showing that this case has very high accuracy and is very reliable when applying this rule to the dataset for prediction).\n",
    "    \n",
    "    <br>\n",
    "    \n",
    "\n",
    "- Finally, i combine all these rules to apply to the classification problem and evaluate them based on metrics such as f1_score, precision, recall, and accuracy.\n",
    "\n",
    "- After running the algorithm, i get:\n",
    "\n",
    "    + Accuracy: 0.8360037700282752\n",
    "\n",
    "    + Precision: 0.43722943722943725\n",
    "\n",
    "    + Recall: 0.696551724137931\n",
    "\n",
    "    + F1_score: 0.5372340425531914\n",
    "\n",
    "    + Observation: Although the precision rate is quite low due to a significant number of false positive predictions, which should be improved by adjusting the rules predicting True. The accuracy and recall rates are high because the dataset contains a majority of False in the \"Churn?\" column (86.3% in this chart above), and the rules predicting False are highly accurate. The F1_score is low due to the imbalance between True and False predictions. In summary, the rules predicting \"Churn? = False\" are reliable and can be applied, while the rules predicting \"Churn? = True\" need adjustments and reconsideration to increase accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FzxGs7RRjaX"
   },
   "source": [
    "# 4 References\n",
    "\n",
    "http://ce.sharif.edu/courses/85-86/1/ce925/assignments/files/assignDir2/ProjectDefinition1.pdf\n",
    "\n",
    "https://cs.wmich.edu/~alfuqaha/summer14/cs6530/lectures/AssociationAnalysis-Part1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "doYH4biqR_N7"
   },
   "source": [
    "Feel free to send questions to my email address: nnduc@fit.hcmus.edu.vn\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab01 - Frequent itemset mining.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
